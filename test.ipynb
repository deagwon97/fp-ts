{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "\n",
    "        #src = [src len, batch size, input_dim]\n",
    "        outputs, (hidden, cell) = self.rnn(src)\n",
    "        \n",
    "        # outputs = [src len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        return hidden, cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim ,input_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "        # Decoder에서 항상 n directions = 1\n",
    "        # 따라서 hidden = [n layers, batch size, hid dim]\n",
    "        # context = [n layers, batch size, hid dim]\n",
    "        # input = [1, batch size]\n",
    "        #input = input.unsqueeze(0)\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        # output = [seq len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        # Decoder에서 항상 seq len = n directions = 1 \n",
    "        # 한 번에 한 토큰씩만 디코딩하므로 seq len = 1\n",
    "        # 따라서 output = [1, batch size, hid dim]\n",
    "        # hidden = [n layers, batch size, hid dim]\n",
    "        # cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        # prediction = [batch size, output dim]\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "   def __init__(self, encoder, decoder, device):\n",
    "       super().__init__()\n",
    "       \n",
    "       self.encoder = encoder\n",
    "       self.decoder = decoder\n",
    "       self.device = device\n",
    "       # Encoder와 Decoder의 hidden dim이 같아야 함\n",
    "       assert encoder.hid_dim == decoder.hid_dim\n",
    "       # Encoder와 Decoder의 layer 개수가 같아야 함\n",
    "       assert encoder.n_layers == decoder.n_layers\n",
    "       \n",
    "   def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "       # src = [src len, batch size]\n",
    "       # trg = [trg len, batch size]\n",
    "       \n",
    "       trg_len = trg.shape[0]\n",
    "       batch_size = trg.shape[1]\n",
    "       trg_vocab_size = self.decoder.ouput_dim\n",
    "       \n",
    "       # decoder 결과를 저장할 텐서\n",
    "       outputs = torch.zeros(trg_len, batch_size, trg_vocab_size)\n",
    "       \n",
    "       # Encoder의 마지막 은닉 상태가 Decoder의 초기 은닉상태로 쓰임\n",
    "       hidden, cell = self.encoder(src)\n",
    "       \n",
    "       # Decoder에 들어갈 첫 input은 <sos> 토큰\n",
    "       input = trg[0, :]\n",
    "       \n",
    "       # target length만큼 반복\n",
    "       # range(0,trg_len)이 아니라 range(1,trg_len)인 이유 : 0번째 trg는 항상 <sos>라서 그에 대한 output도 항상 0 \n",
    "       for t in range(1, trg_len):\n",
    "           output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "           outputs[t] = output\n",
    "           \n",
    "           # random.random() : [0,1] 사이 랜덤한 숫자 \n",
    "           # 랜덤 숫자가 teacher_forcing_ratio보다 작으면 True니까 teacher_force=1\n",
    "           teacher_force = random.random() < teacher_forcing_ratio\n",
    "           \n",
    "           # 확률 가장 높게 예측한 토큰\n",
    "           top1 = output.argmax(1) \n",
    "           \n",
    "           # techer_force = 1 = True이면 trg[t]를 아니면 top1을 input으로 사용\n",
    "           input = trg[t] if teacher_force else top1\n",
    "       \n",
    "       return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(SRC.vocab)\n",
    "output_dim = len(TRG.vocab)\n",
    "\n",
    "# Encoder embedding dim\n",
    "enc_emb_dim = 256\n",
    "# Decoder embedding dim\n",
    "dec_emb_dim = 256\n",
    "\n",
    "hid_dim=512\n",
    "n_layers=2\n",
    "\n",
    "enc_dropout = 0.5\n",
    "dec_dropout=0.5\n",
    "\n",
    "enc = Encoder(input_dim, enc_emb_dim, hid_dim, n_layers, enc_dropout)\n",
    "dec = Decoder(output_dim, dec_emb_dim, hid_dim, n_layer, dec_dropout)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uiform_(param.data, -0.08, 0.08)\n",
    "output = model(src, trg)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  }
 ]
}