{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600271651959",
   "display_name": "Python 3.8.5 64-bit ('deep': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set constant\n",
    "LOC_SIZE = 69\n",
    "TIME_SIZE = 3\n",
    "DATE_SIZE = 241\n",
    "FEATURE_SIZE = 13\n",
    "#set window size\n",
    "INPUT_WINDOW = 20\n",
    "OUTPUT_WINDOW = 7\n",
    "\n",
    "\n",
    "# flow_tot -> flow_trend + flow_cycle 분리\n",
    "def split_trend_cycle(flow_pop):\n",
    "    flow_pop = flow_pop.reshape(LOC_SIZE, DATE_SIZE, 1)\n",
    "    trend = flow_pop\n",
    "    cycle = flow_pop\n",
    "    return np.concatenate([trend, cycle], axis = 2)\n",
    "\n",
    "def df2npy(time_data):\n",
    "    # make loc_list(dong code)\n",
    "    loc_list = list(time_data.HDONG_CD.unique())\n",
    "\n",
    "\n",
    "    # select features\n",
    "    time_data = time_data[['flow_pop', 'HDONG_CD', 'time',\n",
    "                        'card_use', 'holiday', 'day_corona', 'ondo', 'subdo',\n",
    "                        'rain_snow', 'STD_YMD']]\n",
    "\n",
    "    # change string time to int time\n",
    "    time_data.time[time_data.time == 'morning'] = 0 # morning\n",
    "    time_data.time[time_data.time == 'lunch'] = 1 # lunch\n",
    "    time_data.time[time_data.time == 'evening'] = 2 # evening\n",
    "\n",
    "    # to datetime\n",
    "    time_data.STD_YMD = pd.to_datetime(time_data.STD_YMD)\n",
    "\n",
    "    # make dayofyear weekday\n",
    "    time_data['dayofyear'] = time_data.STD_YMD.dt.dayofyear\n",
    "    time_data['weekday'] = time_data.STD_YMD.dt.weekday\n",
    "    time_data['dayofyear_sin'] = np.sin(2 * np.pi * (time_data['dayofyear'])/365)\n",
    "    time_data['dayofyear_cos'] = np.cos(2 * np.pi * (time_data['dayofyear'])/365)\n",
    "    time_data['weekday_sin'] = np.sin(2 * np.pi * (time_data['weekday'])/7)# 월화수목금토일\n",
    "    time_data['weekday_cos'] = np.cos(2 * np.pi * (time_data['weekday'])/7)\n",
    "\n",
    "    # reselect features\n",
    "    time_data = time_data[['HDONG_CD', 'time','flow_pop',\n",
    "                        'card_use', 'holiday', 'day_corona', 'ondo', 'subdo',\n",
    "                        'rain_snow', 'dayofyear_sin', 'dayofyear_cos', 'weekday_sin', 'weekday_cos']]\n",
    "\n",
    "    # table -> matrix\n",
    "    time_data = np.array(time_data).reshape(LOC_SIZE, TIME_SIZE, DATE_SIZE, FEATURE_SIZE)# 지역, 시간, 날짜, features\n",
    "    return time_data\n",
    "\n",
    "def append_trend_cycle(time_data):\n",
    "    time_data_new = np.empty([LOC_SIZE, DATE_SIZE, FEATURE_SIZE + 2])\n",
    "    time_data_new[:,:,:-2] = time_data\n",
    "    # trend, cycle 순서로 return\n",
    "    time_data_new[:,:,-2:] = split_trend_cycle(time_data[:,:,2])# flow_pop의 index는 2\n",
    "    time_data = time_data_new\n",
    "    return time_data\n",
    "\n",
    "def scaleing_time(data, scaler = None):\n",
    "    shape = data.shape\n",
    "    data = data.reshape(-1, shape[-1])\n",
    "    if scaler == None:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(data)\n",
    "    scaled_data = scaler.transform(data)\n",
    "    return scaler, scaled_data.reshape(shape)\n",
    "\n",
    "def scaleing_no_time(data, scaler = None):\n",
    "    df_index = data.index\n",
    "    df_columns = data.columns\n",
    "    if scaler == None:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(data.values.reshape(-1,data.shape[-1]))\n",
    "    data = scaler.transform(data.values.reshape(-1,data.shape[-1]))\n",
    "    data = pd.DataFrame(data, index = df_index, columns = df_columns)\n",
    "    return scaler, data\n",
    "\n",
    "def split_train_valid_test(time_data):\n",
    "    # make_random \n",
    "    loc_index = [i for i in range(69)]\n",
    "    random.seed(1015)\n",
    "    random.shuffle(loc_index)\n",
    "\n",
    "    # split time data\n",
    "    train_time = time_data[loc_index[ :55], :201, :]\n",
    "    valid_time_1 = time_data[loc_index[ :55], 201 -INPUT_WINDOW : -20, :] # train 지역& valid 기간\n",
    "    valid_time_2 = time_data[loc_index[55:62], : -20, :] # valid 지역 & (train + valid) 기간\n",
    "    test_time_1 = time_data[loc_index[:62], 221 -INPUT_WINDOW : , :] # train,valid 지역& test 기간\n",
    "    test_time_2 = time_data[loc_index[62:], :, :] # test 지역 & (train + valid + test) 기간\n",
    "\n",
    "    # set loc index\n",
    "    train_loc_index = list(set(train_time[:,0,0].astype(np.int64)))\n",
    "    valid_loc_index = list(set(valid_time_2[:,0,0].astype(np.int64)))\n",
    "    test_loc_index = list(set(test_time_2[:,0,0].astype(np.int64)))\n",
    "\n",
    "    #scaling - time # 지역별 스케일링\n",
    "    time_scaler, train_time[:,:,2:] = scaleing_time(train_time[:,:,2:])\n",
    "    _, valid_time_1[:,:,2:] = scaleing_time(valid_time_1[:,:,2:], time_scaler)\n",
    "    _, valid_time_2[:,:,2:] = scaleing_time(valid_time_2[:,:,2:], time_scaler)\n",
    "    _, test_time_1[:,:,2:] = scaleing_time(test_time_1[:,:,2:], time_scaler)\n",
    "    _, test_time_2[:,:,2:] = scaleing_time(test_time_2[:,:,2:], time_scaler)\n",
    "\n",
    "    train_valid_test = [train_time, valid_time_1, valid_time_2, test_time_1, test_time_2]\n",
    "    train_valid_test_index = [train_loc_index, valid_loc_index, test_loc_index]\n",
    "\n",
    "    return train_valid_test, train_valid_test_index, time_scaler\n",
    "\n",
    "def split_notime_data(nontime_data, train_valid_test_index):\n",
    "    train_loc_index, valid_loc_index, test_loc_index= train_valid_test_index\n",
    "    # make no time data\n",
    "    nontime_data = nontime_data[['HDONG_CD', 'time', 'tot_pop', 'age_80U', 'AREA']]\n",
    "    nontime_data = nontime_data.groupby(['HDONG_CD']).sum()\n",
    "\n",
    "    # split no time data\n",
    "    train_no_time = nontime_data.loc[train_loc_index]\n",
    "    valid_no_time = nontime_data.loc[valid_loc_index]\n",
    "    test_no_time = nontime_data.loc[test_loc_index]\n",
    "\n",
    "    # scaleing no time data*\n",
    "    no_time_scaler, train_no_time = scaleing_no_time(train_no_time)\n",
    "    _,              valid_no_time = scaleing_no_time(valid_no_time)\n",
    "    _,              test_no_time  = scaleing_no_time(test_no_time)\n",
    "\n",
    "    notime = [train_no_time, valid_no_time, test_no_time]\n",
    "    return no_time_scaler, pd.concat(notime)\n",
    "\n",
    "def split_sequence(sequence, input_window = 20, output_window = 7, target_index  = 2):\n",
    "    x, y = list(), list()\n",
    "    #print(sequence.shape)\n",
    "    for day in range(sequence.shape[0]):\n",
    "        end_ix = day + input_window\n",
    "        if end_ix > (len(sequence)- output_window) -1:#\n",
    "            break\n",
    "        seq_x, seq_y = sequence[day:end_ix, :], sequence[end_ix:end_ix+output_window, 2]\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "def make_time_notime_data(time_data, notime_data, input_window = 20, out_window = 7):\n",
    "    x_time_batch, x_notime_batch, y_batch = list(), list(), list()\n",
    "\n",
    "    x_time = []\n",
    "    x_notime = []\n",
    "    y_time = []\n",
    "\n",
    "    for loc in range(time_data.shape[0]):\n",
    "        loc_code = time_data[loc,0,0]\n",
    "        #print(time_data[loc_code,time_idx,0,0])\n",
    "        x, y = split_sequence(time_data[loc,:,:], input_window, out_window)\n",
    "        notime = notime_data.loc[loc_code]\n",
    "        aug_notime = np.zeros([x.shape[0], notime.shape[0]])\n",
    "        aug_notime[:,:] = notime\n",
    "        \n",
    "        x_time.append(x)\n",
    "        x_notime.append(aug_notime)\n",
    "        y_time.append(y)\n",
    "    \n",
    "    x_time = np.concatenate(x_time)\n",
    "    x_notime = np.concatenate(x_notime)\n",
    "    y_time = np.concatenate(y_time)\n",
    "\n",
    "    print(x_time.shape)\n",
    "    print(x_notime.shape)\n",
    "    print(y_time.shape)\n",
    "\n",
    "    return x_time, x_notime, y_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n morning\n(9570, 20, 15)\n(9570, 3)\n(9570, 7)\n(715, 20, 15)\n(715, 3)\n(715, 7)\n(1358, 20, 15)\n(1358, 3)\n(1358, 7)\n\n lunch\n(9570, 20, 15)\n(9570, 3)\n(9570, 7)\n(715, 20, 15)\n(715, 3)\n(715, 7)\n(1358, 20, 15)\n(1358, 3)\n(1358, 7)\n\n evening\n(9570, 20, 15)\n(9570, 3)\n(9570, 7)\n(715, 20, 15)\n(715, 3)\n(715, 7)\n(1358, 20, 15)\n(1358, 3)\n(1358, 7)\n"
    }
   ],
   "source": [
    "# load data\n",
    "time_data = pd.read_csv('data/original/time_data.txt', sep  = ' ') \n",
    "time_data = df2npy(time_data)\n",
    "\n",
    "# 아침 점심 저녁 분리\n",
    "morning_data = time_data[:,2,:,:]\n",
    "lunch_data = time_data[:,1,:,:]\n",
    "evening_data = time_data[:,0,:,:]\n",
    "\n",
    "# tend, cycle 추가\n",
    "morning_data = append_trend_cycle(morning_data)\n",
    "lunch_data = append_trend_cycle(lunch_data)\n",
    "evening_data = append_trend_cycle(evening_data)\n",
    "\n",
    "# train_validation_test split  & scaling\n",
    "morning_data, train_valid_test_loc_index, m_time_scaler = split_train_valid_test(morning_data)\n",
    "# m_time_train, m_time_valid, m_time_test = morning_data\n",
    "lunch_data, _, l_time_scaler = split_train_valid_test(lunch_data)\n",
    "evening_data, _, e_time_scaler = split_train_valid_test(evening_data)\n",
    "\n",
    "nontime_data = pd.read_csv('data/original/nontime_data.txt', sep = ' ')\n",
    "no_time_scaler, notime = split_notime_data(nontime_data, train_valid_test_loc_index)\n",
    "# notime_train, notime_valid, notime_test = notime\n",
    "\n",
    "print('\\n morning')\n",
    "m_train_time, m_train_notime, m_train_y = make_time_notime_data(morning_data[0], notime)\n",
    "m_valid_time, m_valid_notime, m_valid_y = make_time_notime_data(morning_data[1], notime)\n",
    "m_test_time, m_test_notime, m_test_y = make_time_notime_data(morning_data[2], notime)\n",
    "\n",
    "print('\\n lunch')\n",
    "l_train_time, l_train_notime, l_train_y = make_time_notime_data(lunch_data[0], notime)\n",
    "l_valid_time, l_valid_notime, l_valid_y = make_time_notime_data(lunch_data[1], notime)\n",
    "l_test_time, l_test_notime, l_test_y = make_time_notime_data(lunch_data[2], notime)\n",
    "\n",
    "print('\\n evening')\n",
    "e_train_time, e_train_notime, e_train_y = make_time_notime_data(evening_data[0], notime)\n",
    "e_valid_time, e_valid_notime, e_valid_y = make_time_notime_data(evening_data[1], notime)\n",
    "e_test_time, e_test_notime, e_test_y = make_time_notime_data(evening_data[2], notime)"
   ]
  }
 ]
}